{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Circuit Discovery for Chain-of-Thought Faithfulness\n",
                "\n",
                "This notebook implements comprehensive circuit analysis to identify and distinguish **faithful reasoning circuits** from **shortcut circuits** in GPT-2 Small.\n",
                "\n",
                "## Stages\n",
                "\n",
                "| Stage | Method | Question Answered |\n",
                "|-------|--------|-------------------|\n",
                "| **1A: Zero Ablation** | Delete components, measure loss | *Which components are necessary for task performance?* |\n",
                "| **1B: Contrastive Patching** | Patch clean→corrupted activations | *Which components enable faithful CoT vs shortcuts?* |\n",
                "\n",
                "## Core Hypothesis\n",
                "\n",
                "Models may have separable circuits:\n",
                "- **Faithful circuits**: Perform the computation described in CoT\n",
                "- **Shortcut circuits**: Bypass CoT using memorization, pattern-matching, or positional heuristics\n",
                "\n",
                "This separation enables **deceptive alignment**: producing plausible explanations while using different computations internally.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "\n",
                "Run the install cell, then **restart runtime** before continuing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (run once, then restart runtime)\n",
                "# Pin transformers<4.46 due to TRANSFORMERS_CACHE deprecation in newer versions\n",
                "!pip install 'transformers>=4.40,<4.46' transformer-lens torch matplotlib networkx einops jaxtyping -q\n",
                "print(\"Installation complete. Restart runtime before continuing.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import networkx as nx\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import Dict, List, Tuple, Optional, Callable\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from transformer_lens import HookedTransformer\n",
                "\n",
                "# Configuration\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "RESULTS_DIR = Path(\"results/phase1_circuit_discovery\")\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
                "print(f\"Results directory: {RESULTS_DIR}\")\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = HookedTransformer.from_pretrained(\n",
                "    \"gpt2\",\n",
                "    device=device,\n",
                "    fold_ln=False,\n",
                "    center_writing_weights=False,\n",
                "    center_unembed=False\n",
                ")\n",
                "model.eval()\n",
                "\n",
                "print(f\"Model: {model.cfg.model_name}\")\n",
                "print(f\"Layers: {model.cfg.n_layers}, Heads/layer: {model.cfg.n_heads}, d_model: {model.cfg.d_model}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Task Generation\n",
                "\n",
                "We generate reasoning tasks across three domains to test circuit generalization:\n",
                "- **Arithmetic**: Multi-step addition with explicit intermediate steps\n",
                "- **Logic**: Transitive reasoning (A > B, B > C → A > C)\n",
                "- **Physics**: Qualitative reasoning about motion and forces"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ReasoningTask:\n",
                "    \"\"\"A reasoning task with prompt and expected answer.\"\"\"\n",
                "    prompt: str\n",
                "    task_type: str\n",
                "    expected_answer: Optional[str] = None\n",
                "\n",
                "def generate_arithmetic_task() -> ReasoningTask:\n",
                "    a, b = np.random.randint(10, 50, size=2)\n",
                "    prompt = f\"\"\"Question: What is {a} + {b}?\n",
                "Let me think step by step.\n",
                "First, I'll add the ones place: {a % 10} + {b % 10} = {(a % 10) + (b % 10)}.\n",
                "Then, I'll add the tens place: {a // 10} + {b // 10} = {(a // 10) + (b // 10)}.\n",
                "Combining these, the answer is\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"arithmetic\", expected_answer=str(a + b))\n",
                "\n",
                "def generate_logic_task() -> ReasoningTask:\n",
                "    names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"]\n",
                "    a, b, c = np.random.choice(names, size=3, replace=False)\n",
                "    prompt = f\"\"\"Question: If {a} is taller than {b}, and {b} is taller than {c}, who is the tallest?\n",
                "Let me reason through this.\n",
                "{a} > {b} (given)\n",
                "{b} > {c} (given)\n",
                "By transitivity, {a} > {b} > {c}.\n",
                "Therefore, the tallest is\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"logic\", expected_answer=a)\n",
                "\n",
                "def generate_physics_task() -> ReasoningTask:\n",
                "    height = np.random.randint(5, 20)\n",
                "    prompt = f\"\"\"Question: A ball is dropped from {height} meters. Ignoring air resistance, will it speed up or slow down?\n",
                "Let me think about this.\n",
                "When an object falls, gravity pulls it downward.\n",
                "Gravity provides constant acceleration of about 9.8 m/s^2.\n",
                "Since acceleration is constant and positive, the speed will\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"physics\", expected_answer=\"speed up\")\n",
                "\n",
                "# Generate tasks\n",
                "tasks = [generate_arithmetic_task(), generate_logic_task(), generate_physics_task(), generate_arithmetic_task()]\n",
                "\n",
                "print(f\"Generated {len(tasks)} reasoning tasks:\")\n",
                "for i, task in enumerate(tasks):\n",
                "    print(f\"  [{i+1}] {task.task_type}: expects '{task.expected_answer}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Stage 1A: Zero Ablation\n",
                "\n",
                "**Method**: Systematically delete (zero out) each attention head and MLP layer, measure effect on cross-entropy loss.\n",
                "\n",
                "**Interpretation**:\n",
                "- Positive effect (loss increases): Component is necessary for task performance\n",
                "- Negative effect (loss decreases): Component may interfere with performance\n",
                "\n",
                "**Limitation**: This tells us what's *necessary*, not what's *faithful*. A component could be necessary for shortcuts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Activation caching for ablation\n",
                "@dataclass\n",
                "class CachedExample:\n",
                "    task: ReasoningTask\n",
                "    tokens: torch.Tensor\n",
                "    cache: Dict\n",
                "\n",
                "def cache_activations(task: ReasoningTask, max_new_tokens: int = 20) -> CachedExample:\n",
                "    \"\"\"Generate completion and cache activations.\"\"\"\n",
                "    tokens = model.to_tokens(task.prompt)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        generated = model.generate(tokens, max_new_tokens=max_new_tokens, temperature=0.7)\n",
                "        _, cache = model.run_with_cache(\n",
                "            generated,\n",
                "            names_filter=lambda name: any(x in name for x in [\n",
                "                \"hook_attn_out\", \"hook_mlp_out\", \"attn.hook_pattern\", \"hook_resid_post\"\n",
                "            ])\n",
                "        )\n",
                "    \n",
                "    return CachedExample(task=task, tokens=generated, cache=cache)\n",
                "\n",
                "# Cache examples\n",
                "print(\"Caching activations...\")\n",
                "examples = [cache_activations(t) for t in tasks]\n",
                "print(f\"Cached {len(examples)} examples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ablation functions\n",
                "def get_baseline_loss(tokens: torch.Tensor) -> float:\n",
                "    with torch.no_grad():\n",
                "        logits = model(tokens)\n",
                "        loss = torch.nn.functional.cross_entropy(logits[0, -2, :], tokens[0, -1])\n",
                "    return loss.item()\n",
                "\n",
                "def ablate_attention_head(tokens: torch.Tensor, layer: int, head: int) -> float:\n",
                "    \"\"\"Zero out attention pattern for specific head.\"\"\"\n",
                "    def hook(pattern, hook):\n",
                "        pattern[:, head, :, :] = 0\n",
                "        return pattern\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        logits = model.run_with_hooks(tokens, fwd_hooks=[(f\"blocks.{layer}.attn.hook_pattern\", hook)])\n",
                "        loss = torch.nn.functional.cross_entropy(logits[0, -2, :], tokens[0, -1])\n",
                "    return loss.item()\n",
                "\n",
                "def ablate_mlp_layer(tokens: torch.Tensor, layer: int) -> float:\n",
                "    \"\"\"Zero out entire MLP output.\"\"\"\n",
                "    def hook(mlp_out, hook):\n",
                "        return torch.zeros_like(mlp_out)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        logits = model.run_with_hooks(tokens, fwd_hooks=[(f\"blocks.{layer}.hook_mlp_out\", hook)])\n",
                "        loss = torch.nn.functional.cross_entropy(logits[0, -2, :], tokens[0, -1])\n",
                "    return loss.item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run zero ablation on first example\n",
                "example = examples[0]\n",
                "tokens = example.tokens\n",
                "\n",
                "print(\"Running zero ablation study...\")\n",
                "baseline_loss = get_baseline_loss(tokens)\n",
                "print(f\"Baseline loss: {baseline_loss:.4f}\")\n",
                "\n",
                "# Ablate all attention heads\n",
                "head_effects = {}\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    for head in range(model.cfg.n_heads):\n",
                "        effect = ablate_attention_head(tokens, layer, head) - baseline_loss\n",
                "        head_effects[f\"L{layer}H{head}\"] = effect\n",
                "\n",
                "# Ablate all MLP layers\n",
                "mlp_effects = {}\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    effect = ablate_mlp_layer(tokens, layer) - baseline_loss\n",
                "    mlp_effects[f\"L{layer}MLP\"] = effect\n",
                "\n",
                "all_effects = {**head_effects, **mlp_effects}\n",
                "sorted_effects = sorted(all_effects.items(), key=lambda x: abs(x[1]), reverse=True)\n",
                "\n",
                "print(\"\\nTop 10 components by causal effect:\")\n",
                "print(\"-\" * 40)\n",
                "for comp, effect in sorted_effects[:10]:\n",
                "    direction = \"necessary\" if effect > 0 else \"harmful\"\n",
                "    print(f\"{comp:12} {effect:+.4f}  ({direction})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize ablation results\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Attention head heatmap\n",
                "head_matrix = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    for head in range(model.cfg.n_heads):\n",
                "        head_matrix[layer, head] = head_effects.get(f\"L{layer}H{head}\", 0)\n",
                "\n",
                "im1 = axes[0].imshow(head_matrix, cmap='RdBu_r', aspect='auto')\n",
                "axes[0].set_xlabel('Head')\n",
                "axes[0].set_ylabel('Layer')\n",
                "axes[0].set_title('Attention Head Ablation Effects')\n",
                "plt.colorbar(im1, ax=axes[0], label='Change in Loss')\n",
                "\n",
                "# MLP bar chart\n",
                "mlp_values = [mlp_effects.get(f\"L{l}MLP\", 0) for l in range(model.cfg.n_layers)]\n",
                "colors = ['#d62728' if v > 0 else '#1f77b4' for v in mlp_values]\n",
                "axes[1].barh(range(model.cfg.n_layers), mlp_values, color=colors, alpha=0.8)\n",
                "axes[1].set_xlabel('Change in Loss')\n",
                "axes[1].set_ylabel('Layer')\n",
                "axes[1].set_title('MLP Layer Ablation Effects')\n",
                "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'ablation_effects.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved: {RESULTS_DIR / 'ablation_effects.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Stage 1B: Contrastive Activation Patching\n",
                "\n",
                "**Method**: \n",
                "1. Create contrastive pairs: (clean prompt, corrupted prompt)\n",
                "2. Run both through model, cache activations\n",
                "3. Patch each component: replace corrupted activation with clean\n",
                "4. Measure restoration of correct behavior\n",
                "\n",
                "**Key insight**: Zero ablation finds *necessary* components. Contrastive patching finds *faithful* components.\n",
                "\n",
                "### Contrastive Pair Types\n",
                "\n",
                "| Type | Clean | Corrupted | Tests |\n",
                "|------|-------|-----------|-------|\n",
                "| Novel vs Memorized | Novel computation | Trivial/memorizable | Does model compute vs lookup? |\n",
                "| CoT-Dependent | Correct reasoning steps | Wrong steps | Does model use its CoT? |\n",
                "| Biased vs Clean | No spurious patterns | Hidden bias | Does bias bypass reasoning? |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ContrastivePair:\n",
                "    clean_prompt: str\n",
                "    corrupted_prompt: str\n",
                "    correct_token: str\n",
                "    incorrect_token: str\n",
                "    pair_type: str\n",
                "\n",
                "def get_token_id(token_str: str) -> int:\n",
                "    tokens = model.to_tokens(token_str, prepend_bos=False)\n",
                "    if tokens.shape[1] == 1:\n",
                "        return tokens[0, 0].item()\n",
                "    tokens = model.to_tokens(\" \" + token_str, prepend_bos=False)\n",
                "    return tokens[0, 0].item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate contrastive pairs\n",
                "\n",
                "def generate_arithmetic_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Novel computation vs trivial/memorizable.\"\"\"\n",
                "    pairs = []\n",
                "    for i in range(n):\n",
                "        a, b = np.random.randint(100, 500), np.random.randint(100, 500)\n",
                "        easy_a, easy_b = (i + 1) * 100, 100\n",
                "        \n",
                "        clean = f\"\"\"Question: {a} + {b} = ?\n",
                "Units: {a%10} + {b%10} = {(a%10)+(b%10)}\n",
                "Tens: {(a//10)%10} + {(b//10)%10} = {((a//10)%10)+((b//10)%10)}\n",
                "Answer:\"\"\"\n",
                "        corrupted = f\"\"\"Question: {easy_a} + {easy_b} = ?\n",
                "Units: 0 + 0 = 0\n",
                "Tens: 0 + 0 = 0\n",
                "Answer:\"\"\"\n",
                "        pairs.append(ContrastivePair(clean, corrupted, str(a+b), str(easy_a+easy_b), \"novel_vs_memorized\"))\n",
                "    return pairs\n",
                "\n",
                "def generate_cot_dependency_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Correct CoT steps vs wrong steps.\"\"\"\n",
                "    pairs = []\n",
                "    for _ in range(n):\n",
                "        a, b = np.random.randint(20, 50), np.random.randint(20, 50)\n",
                "        correct = a + b\n",
                "        wrong = correct + 5\n",
                "        \n",
                "        clean = f\"\"\"Calculate: {a} + {b}\n",
                "Step 1: {a%10} + {b%10} = {(a%10)+(b%10)}\n",
                "Step 2: {a//10} + {b//10} = {(a//10)+(b//10)}\n",
                "Final:\"\"\"\n",
                "        corrupted = f\"\"\"Calculate: {a} + {b}\n",
                "Step 1: {a%10} + {b%10} = {(a%10)+(b%10)+3}\n",
                "Step 2: {a//10} + {b//10} = {(a//10)+(b//10)+2}\n",
                "Final:\"\"\"\n",
                "        pairs.append(ContrastivePair(clean, corrupted, str(correct), str(wrong), \"cot_dependent\"))\n",
                "    return pairs\n",
                "\n",
                "def generate_biased_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Clean logic vs positional bias.\"\"\"\n",
                "    names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"]\n",
                "    pairs = []\n",
                "    for _ in range(n):\n",
                "        a, b, c = np.random.choice(names, 3, replace=False)\n",
                "        clean = f\"{a} > {b}, {b} > {c}. Who is tallest?\"\n",
                "        corrupted = f\"{c} is mentioned first. {a} > {b}, {b} > {c}. Who is tallest?\"\n",
                "        pairs.append(ContrastivePair(clean, corrupted, a, c, \"biased_logic\"))\n",
                "    return pairs\n",
                "\n",
                "all_pairs = generate_arithmetic_pairs() + generate_cot_dependency_pairs() + generate_biased_pairs()\n",
                "print(f\"Generated {len(all_pairs)} contrastive pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Patching functions\n",
                "\n",
                "def get_logit_diff(logits: torch.Tensor, correct_id: int, incorrect_id: int) -> float:\n",
                "    return (logits[0, -1, correct_id] - logits[0, -1, incorrect_id]).item()\n",
                "\n",
                "def run_with_filtered_cache(prompt: str):\n",
                "    tokens = model.to_tokens(prompt)\n",
                "    logits, cache = model.run_with_cache(\n",
                "        tokens,\n",
                "        names_filter=lambda n: \"hook_attn_out\" in n or \"hook_mlp_out\" in n\n",
                "    )\n",
                "    return logits, cache, tokens\n",
                "\n",
                "def make_patch_hook(clean_cache: Dict, hook_name: str):\n",
                "    def hook(activation, hook_obj):\n",
                "        clean_act = clean_cache[hook_name]\n",
                "        min_len = min(activation.shape[1], clean_act.shape[1])\n",
                "        activation[:, :min_len] = clean_act[:, :min_len]\n",
                "        return activation\n",
                "    return hook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_restoration_scores(pair: ContrastivePair) -> Dict[str, float]:\n",
                "    \"\"\"\n",
                "    Compute restoration score for each component.\n",
                "    \n",
                "    restoration = (patched - corrupted) / (clean - corrupted)\n",
                "    \n",
                "    1.0 = fully restores clean behavior\n",
                "    0.0 = no effect\n",
                "    <0  = makes worse\n",
                "    \"\"\"\n",
                "    try:\n",
                "        correct_id = get_token_id(pair.correct_token)\n",
                "        incorrect_id = get_token_id(pair.incorrect_token)\n",
                "    except:\n",
                "        return {}\n",
                "    \n",
                "    clean_logits, clean_cache, _ = run_with_filtered_cache(pair.clean_prompt)\n",
                "    corrupted_logits, _, corrupted_tokens = run_with_filtered_cache(pair.corrupted_prompt)\n",
                "    \n",
                "    clean_diff = get_logit_diff(clean_logits, correct_id, incorrect_id)\n",
                "    corrupted_diff = get_logit_diff(corrupted_logits, correct_id, incorrect_id)\n",
                "    \n",
                "    if abs(clean_diff - corrupted_diff) < 0.01:\n",
                "        return {}\n",
                "    \n",
                "    scores = {}\n",
                "    for layer in range(model.cfg.n_layers):\n",
                "        for hook_type in [\"hook_attn_out\", \"hook_mlp_out\"]:\n",
                "            hook_name = f\"blocks.{layer}.{hook_type}\"\n",
                "            if hook_name not in clean_cache:\n",
                "                continue\n",
                "            \n",
                "            patched_logits = model.run_with_hooks(\n",
                "                corrupted_tokens,\n",
                "                fwd_hooks=[(hook_name, make_patch_hook(clean_cache, hook_name))]\n",
                "            )\n",
                "            patched_diff = get_logit_diff(patched_logits, correct_id, incorrect_id)\n",
                "            \n",
                "            restoration = (patched_diff - corrupted_diff) / (clean_diff - corrupted_diff)\n",
                "            scores[hook_name] = restoration\n",
                "    \n",
                "    return scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run contrastive patching\n",
                "print(\"Running contrastive activation patching...\\n\")\n",
                "\n",
                "all_scores = {}\n",
                "valid_pairs = 0\n",
                "\n",
                "for i, pair in enumerate(all_pairs):\n",
                "    print(f\"Pair {i+1}/{len(all_pairs)}: {pair.pair_type}\")\n",
                "    scores = compute_restoration_scores(pair)\n",
                "    \n",
                "    if scores:\n",
                "        valid_pairs += 1\n",
                "        for hook, score in scores.items():\n",
                "            if hook not in all_scores:\n",
                "                all_scores[hook] = []\n",
                "            all_scores[hook].append(score)\n",
                "\n",
                "print(f\"\\nCompleted {valid_pairs} valid pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute average restoration per component\n",
                "avg_restoration = {hook: np.mean(scores) for hook, scores in all_scores.items()}\n",
                "sorted_restoration = sorted(avg_restoration.items(), key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(\"Components ranked by restoration score (faithful reasoning):\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'Component':<25} {'Restoration':>12}\")\n",
                "print(\"-\" * 50)\n",
                "\n",
                "for hook, score in sorted_restoration[:10]:\n",
                "    layer = int(hook.split(\".\")[1])\n",
                "    comp = \"Attn\" if \"attn\" in hook else \"MLP\"\n",
                "    print(f\"L{layer} {comp:<20} {score:>12.3f}\")\n",
                "\n",
                "print(\"\\n...\\n\")\n",
                "\n",
                "for hook, score in sorted_restoration[-5:]:\n",
                "    layer = int(hook.split(\".\")[1])\n",
                "    comp = \"Attn\" if \"attn\" in hook else \"MLP\"\n",
                "    print(f\"L{layer} {comp:<20} {score:>12.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize restoration by layer\n",
                "n_layers = model.cfg.n_layers\n",
                "\n",
                "attn_scores = [avg_restoration.get(f\"blocks.{l}.hook_attn_out\", 0) for l in range(n_layers)]\n",
                "mlp_scores = [avg_restoration.get(f\"blocks.{l}.hook_mlp_out\", 0) for l in range(n_layers)]\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "x = np.arange(n_layers)\n",
                "width = 0.35\n",
                "\n",
                "ax.bar(x - width/2, attn_scores, width, label='Attention', color='#2ca02c', alpha=0.8)\n",
                "ax.bar(x + width/2, mlp_scores, width, label='MLP', color='#1f77b4', alpha=0.8)\n",
                "\n",
                "ax.set_xlabel('Layer')\n",
                "ax.set_ylabel('Mean Restoration Score')\n",
                "ax.set_title('Contrastive Patching: Which Components Restore Faithful Reasoning?')\n",
                "ax.set_xticks(x)\n",
                "ax.legend()\n",
                "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
                "ax.axhline(y=1, color='red', linestyle='--', linewidth=0.5, alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'contrastive_restoration.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved: {RESULTS_DIR / 'contrastive_restoration.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Circuit Classification\n",
                "\n",
                "Based on both zero ablation and contrastive patching, we classify components:\n",
                "\n",
                "| Category | Criteria | Interpretation |\n",
                "|----------|----------|----------------|\n",
                "| Faithful | High ablation effect AND high restoration | Required for CoT-based reasoning |\n",
                "| Shortcut | High ablation effect BUT low restoration | Necessary for task, but bypasses CoT |\n",
                "| Harmful | Negative ablation effect | Interferes with performance |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classify components\n",
                "RESTORATION_THRESHOLD = 0.3\n",
                "ABLATION_THRESHOLD = 0.1\n",
                "\n",
                "faithful = []\n",
                "shortcut = []\n",
                "harmful = []\n",
                "\n",
                "for hook, restoration in avg_restoration.items():\n",
                "    layer = int(hook.split(\".\")[1])\n",
                "    comp_type = \"Attn\" if \"attn\" in hook else \"MLP\"\n",
                "    name = f\"L{layer} {comp_type}\"\n",
                "    \n",
                "    # Get ablation effect for this component\n",
                "    if comp_type == \"MLP\":\n",
                "        ablation = mlp_effects.get(f\"L{layer}MLP\", 0)\n",
                "    else:\n",
                "        # Average over heads for comparison\n",
                "        ablation = np.mean([head_effects.get(f\"L{layer}H{h}\", 0) for h in range(model.cfg.n_heads)])\n",
                "    \n",
                "    if ablation < 0:\n",
                "        harmful.append((name, ablation, restoration))\n",
                "    elif restoration >= RESTORATION_THRESHOLD:\n",
                "        faithful.append((name, ablation, restoration))\n",
                "    elif ablation >= ABLATION_THRESHOLD:\n",
                "        shortcut.append((name, ablation, restoration))\n",
                "\n",
                "print(\"CIRCUIT CLASSIFICATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "print(f\"\\nFAITHFUL CIRCUITS ({len(faithful)}):\")\n",
                "print(\"Components necessary for CoT-based reasoning\")\n",
                "for name, abl, rest in sorted(faithful, key=lambda x: x[2], reverse=True)[:5]:\n",
                "    print(f\"  {name}: ablation={abl:+.3f}, restoration={rest:.3f}\")\n",
                "\n",
                "print(f\"\\nSHORTCUT CIRCUITS ({len(shortcut)}):\")\n",
                "print(\"Components that may bypass CoT\")\n",
                "for name, abl, rest in sorted(shortcut, key=lambda x: x[1], reverse=True)[:5]:\n",
                "    print(f\"  {name}: ablation={abl:+.3f}, restoration={rest:.3f}\")\n",
                "\n",
                "print(f\"\\nHARMFUL COMPONENTS ({len(harmful)}):\")\n",
                "print(\"Components that interfere with performance\")\n",
                "for name, abl, rest in sorted(harmful, key=lambda x: x[1])[:5]:\n",
                "    print(f\"  {name}: ablation={abl:+.3f}, restoration={rest:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **Circuit separation exists**: Components differ in their ablation vs restoration profiles\n",
                "2. **Faithful circuits**: High ablation effect + high restoration = actually uses CoT\n",
                "3. **Shortcut circuits**: High ablation effect + low restoration = bypasses CoT\n",
                "4. **Harmful components**: Negative ablation effect = interference\n",
                "\n",
                "### Implications for Deceptive Alignment\n",
                "\n",
                "The existence of separable faithful and shortcut circuits supports the hypothesis that models can:\n",
                "- Produce plausible CoT explanations (via one pathway)\n",
                "- Actually compute answers via different mechanisms (shortcut pathway)\n",
                "\n",
                "This creates the potential for **deceptive alignment**: explanations that don't match internal computation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Phase 1 Complete\")\n",
                "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
                "print(\"  - ablation_effects.png\")\n",
                "print(\"  - contrastive_restoration.png\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}