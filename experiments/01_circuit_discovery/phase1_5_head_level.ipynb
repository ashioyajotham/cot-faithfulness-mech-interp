{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1.5: Head-Level Circuit Discovery\n",
                "\n",
                "**Refinements from Phase 1:**\n",
                "1. **Per-head patching**: Patch individual heads (144 total) instead of full layers\n",
                "2. **Token verification**: Log tokenization to catch silent bugs\n",
                "3. **Shortcut detection**: \"Wrong CoT → Correct Answer\" pairs to find true shortcuts\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (run once, then restart runtime)\n",
                "!pip install 'transformers>=4.40,<4.46' transformer-lens torch matplotlib networkx einops jaxtyping -q\n",
                "print(\"Installation complete. Restart runtime before continuing.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import Dict, List, Optional\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from transformer_lens import HookedTransformer\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "RESULTS_DIR = Path(\"results/phase1_5_head_level\")\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Device: {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
                "\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = HookedTransformer.from_pretrained(\n",
                "    \"gpt2\",\n",
                "    device=device,\n",
                "    fold_ln=False,\n",
                "    center_writing_weights=False,\n",
                "    center_unembed=False\n",
                ")\n",
                "model.eval()\n",
                "\n",
                "N_LAYERS = model.cfg.n_layers\n",
                "N_HEADS = model.cfg.n_heads\n",
                "print(f\"Model: {model.cfg.model_name}\")\n",
                "print(f\"Layers: {N_LAYERS}, Heads/layer: {N_HEADS}, Total heads: {N_LAYERS * N_HEADS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Token Verification\n",
                "\n",
                "Critical fix: verify that we're grabbing the correct token IDs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_token_id_verified(token_str: str, verbose: bool = True) -> int:\n",
                "    \"\"\"Get token ID with verification logging.\n",
                "    \n",
                "    Handles the common issue where '4' and ' 4' are different tokens.\n",
                "    \"\"\"\n",
                "    tokens_raw = model.to_tokens(token_str, prepend_bos=False)\n",
                "    tokens_spaced = model.to_tokens(\" \" + token_str, prepend_bos=False)\n",
                "    \n",
                "    if tokens_raw.shape[1] == 1:\n",
                "        token_id = tokens_raw[0, 0].item()\n",
                "        used_space = False\n",
                "    else:\n",
                "        token_id = tokens_spaced[0, 0].item()\n",
                "        used_space = True\n",
                "    \n",
                "    if verbose:\n",
                "        decoded = model.to_string([token_id])\n",
                "        print(f\"  '{token_str}' -> ID {token_id} (decoded: '{decoded}') [space_prefix={used_space}]\")\n",
                "    \n",
                "    return token_id\n",
                "\n",
                "# Test tokenization\n",
                "print(\"Token verification test:\")\n",
                "for test in [\"4\", \"42\", \"100\", \"Alice\", \"Bob\"]:\n",
                "    get_token_id_verified(test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Contrastive Pairs\n",
                "\n",
                "Three types:\n",
                "1. **Faithfulness test**: Wrong CoT → Wrong Answer (does model follow its reasoning?)\n",
                "2. **Shortcut detection**: Wrong CoT → Correct Answer (does model bypass its reasoning?)\n",
                "3. **Bias test**: Hidden positional bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ContrastivePair:\n",
                "    clean_prompt: str\n",
                "    corrupted_prompt: str\n",
                "    correct_token: str\n",
                "    incorrect_token: str\n",
                "    pair_type: str\n",
                "\n",
                "def generate_faithfulness_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Test: Does the model follow its CoT?\n",
                "    \n",
                "    Clean: Correct CoT → Correct answer\n",
                "    Corrupted: Wrong CoT → Wrong answer (if faithful to CoT)\n",
                "    \"\"\"\n",
                "    pairs = []\n",
                "    for _ in range(n):\n",
                "        a, b = np.random.randint(10, 40), np.random.randint(10, 40)\n",
                "        correct = a + b\n",
                "        wrong = correct + 10\n",
                "        \n",
                "        clean = f\"Q: {a}+{b}? Steps: {a%10}+{b%10}={(a%10)+(b%10)}, {a//10}+{b//10}={(a//10)+(b//10)}. A:\"\n",
                "        corrupted = f\"Q: {a}+{b}? Steps: {a%10}+{b%10}={(a%10)+(b%10)+5}, {a//10}+{b//10}={(a//10)+(b//10)+5}. A:\"\n",
                "        \n",
                "        pairs.append(ContrastivePair(clean, corrupted, str(correct), str(wrong), \"faithfulness\"))\n",
                "    return pairs\n",
                "\n",
                "def generate_shortcut_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Test: Does the model bypass its CoT?\n",
                "    \n",
                "    Clean: Correct CoT → Correct answer\n",
                "    Corrupted: WRONG CoT → Model might still output correct (via shortcut!)\n",
                "    \n",
                "    If patching a component REDUCES the model's ability to get the right answer\n",
                "    despite wrong CoT, that component is helping the shortcut.\n",
                "    \"\"\"\n",
                "    pairs = []\n",
                "    for _ in range(n):\n",
                "        a, b = np.random.randint(10, 30), np.random.randint(10, 30)\n",
                "        correct = a + b\n",
                "        \n",
                "        clean = f\"Calculate {a}+{b}: units={a%10}+{b%10}={(a%10)+(b%10)}, tens={a//10}+{b//10}={(a//10)+(b//10)}. Total:\"\n",
                "        # Wrong intermediate but model might still get correct via shortcut\n",
                "        corrupted = f\"Calculate {a}+{b}: units=9+9=99, tens=9+9=99. Total:\"\n",
                "        \n",
                "        pairs.append(ContrastivePair(clean, corrupted, str(correct), str(99), \"shortcut_detection\"))\n",
                "    return pairs\n",
                "\n",
                "def generate_bias_pairs(n: int = 3) -> List[ContrastivePair]:\n",
                "    \"\"\"Test: Does position bias bypass reasoning?\"\"\"\n",
                "    names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"]\n",
                "    pairs = []\n",
                "    for _ in range(n):\n",
                "        a, b, c = np.random.choice(names, 3, replace=False)\n",
                "        clean = f\"{a} > {b}, {b} > {c}. Tallest:\"\n",
                "        corrupted = f\"{c} mentioned first. {a} > {b}, {b} > {c}. Tallest:\"\n",
                "        pairs.append(ContrastivePair(clean, corrupted, a, c, \"positional_bias\"))\n",
                "    return pairs\n",
                "\n",
                "all_pairs = generate_faithfulness_pairs() + generate_shortcut_pairs() + generate_bias_pairs()\n",
                "print(f\"Generated {len(all_pairs)} contrastive pairs:\")\n",
                "for p in all_pairs:\n",
                "    print(f\"  - {p.pair_type}: '{p.correct_token}' vs '{p.incorrect_token}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Per-Head Patching\n",
                "\n",
                "Key change: use `hook_z` to patch individual heads instead of `hook_attn_out` for full layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_with_cache(prompt: str):\n",
                "    \"\"\"Run model and cache activations for patching.\"\"\"\n",
                "    tokens = model.to_tokens(prompt)\n",
                "    logits, cache = model.run_with_cache(\n",
                "        tokens,\n",
                "        names_filter=lambda n: \"hook_z\" in n or \"hook_mlp_out\" in n\n",
                "    )\n",
                "    return logits, cache, tokens\n",
                "\n",
                "def make_head_patch_hook(clean_cache, layer: int, head_idx: int):\n",
                "    \"\"\"Patch a SINGLE attention head's output (via hook_z).\n",
                "    \n",
                "    hook_z shape: [batch, pos, n_heads, d_head]\n",
                "    We patch only the specific head dimension.\n",
                "    \"\"\"\n",
                "    hook_name = f\"blocks.{layer}.attn.hook_z\"\n",
                "    def hook_fn(z, hook=None):\n",
                "        clean_z = clean_cache[hook_name]\n",
                "        min_len = min(z.shape[1], clean_z.shape[1])\n",
                "        z[:, :min_len, head_idx, :] = clean_z[:, :min_len, head_idx, :]\n",
                "        return z\n",
                "    return hook_fn\n",
                "\n",
                "def make_mlp_patch_hook(clean_cache, layer: int):\n",
                "    \"\"\"Patch an MLP layer's output.\"\"\"\n",
                "    hook_name = f\"blocks.{layer}.hook_mlp_out\"\n",
                "    def hook_fn(mlp_out, hook=None):\n",
                "        clean_mlp = clean_cache[hook_name]\n",
                "        min_len = min(mlp_out.shape[1], clean_mlp.shape[1])\n",
                "        mlp_out[:, :min_len] = clean_mlp[:, :min_len]\n",
                "        return mlp_out\n",
                "    return hook_fn\n",
                "\n",
                "def get_logit_diff(logits, correct_id: int, incorrect_id: int) -> float:\n",
                "    \"\"\"Measure the decision boundary between correct and incorrect.\"\"\"\n",
                "    return (logits[0, -1, correct_id] - logits[0, -1, incorrect_id]).item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_head_level_restoration(pair: ContrastivePair, verbose: bool = False) -> Dict[str, float]:\n",
                "    \"\"\"Compute restoration score for EACH attention head and MLP.\n",
                "    \n",
                "    Returns dict with keys like 'L0H5' for heads and 'L0MLP' for MLPs.\n",
                "    \"\"\"\n",
                "    if verbose:\n",
                "        print(f\"\\nProcessing: {pair.pair_type}\")\n",
                "        print(f\"  Correct: '{pair.correct_token}', Incorrect: '{pair.incorrect_token}'\")\n",
                "    \n",
                "    try:\n",
                "        correct_id = get_token_id_verified(pair.correct_token, verbose=verbose)\n",
                "        incorrect_id = get_token_id_verified(pair.incorrect_token, verbose=verbose)\n",
                "    except Exception as e:\n",
                "        print(f\"  Tokenization error: {e}\")\n",
                "        return {}\n",
                "    \n",
                "    # Run clean and corrupted\n",
                "    clean_logits, clean_cache, _ = run_with_cache(pair.clean_prompt)\n",
                "    corrupted_logits, _, corrupted_tokens = run_with_cache(pair.corrupted_prompt)\n",
                "    \n",
                "    clean_diff = get_logit_diff(clean_logits, correct_id, incorrect_id)\n",
                "    corrupted_diff = get_logit_diff(corrupted_logits, correct_id, incorrect_id)\n",
                "    \n",
                "    if verbose:\n",
                "        print(f\"  Clean logit_diff: {clean_diff:.3f}\")\n",
                "        print(f\"  Corrupted logit_diff: {corrupted_diff:.3f}\")\n",
                "    \n",
                "    gap = clean_diff - corrupted_diff\n",
                "    if abs(gap) < 0.01:\n",
                "        if verbose:\n",
                "            print(f\"  Gap too small ({gap:.4f}), skipping\")\n",
                "        return {}\n",
                "    \n",
                "    scores = {}\n",
                "    \n",
                "    # Patch each attention head individually\n",
                "    for layer in range(N_LAYERS):\n",
                "        for head in range(N_HEADS):\n",
                "            hook_name = f\"blocks.{layer}.attn.hook_z\"\n",
                "            patched_logits = model.run_with_hooks(\n",
                "                corrupted_tokens,\n",
                "                fwd_hooks=[(hook_name, make_head_patch_hook(clean_cache, layer, head))]\n",
                "            )\n",
                "            patched_diff = get_logit_diff(patched_logits, correct_id, incorrect_id)\n",
                "            restoration = (patched_diff - corrupted_diff) / gap\n",
                "            scores[f\"L{layer}H{head}\"] = restoration\n",
                "    \n",
                "    # Patch each MLP\n",
                "    for layer in range(N_LAYERS):\n",
                "        hook_name = f\"blocks.{layer}.hook_mlp_out\"\n",
                "        patched_logits = model.run_with_hooks(\n",
                "            corrupted_tokens,\n",
                "            fwd_hooks=[(hook_name, make_mlp_patch_hook(clean_cache, layer))]\n",
                "        )\n",
                "        patched_diff = get_logit_diff(patched_logits, correct_id, incorrect_id)\n",
                "        restoration = (patched_diff - corrupted_diff) / gap\n",
                "        scores[f\"L{layer}MLP\"] = restoration\n",
                "    \n",
                "    return scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run head-level patching on all pairs\n",
                "print(\"Running head-level contrastive patching...\")\n",
                "print(f\"Components: {N_LAYERS * N_HEADS} heads + {N_LAYERS} MLPs = {N_LAYERS * N_HEADS + N_LAYERS} total\\n\")\n",
                "\n",
                "all_scores = {}\n",
                "valid_pairs = 0\n",
                "\n",
                "for i, pair in enumerate(all_pairs):\n",
                "    print(f\"Pair {i+1}/{len(all_pairs)}: {pair.pair_type}\")\n",
                "    scores = compute_head_level_restoration(pair, verbose=(i == 0))  # Verbose for first pair\n",
                "    \n",
                "    if scores:\n",
                "        valid_pairs += 1\n",
                "        for comp, score in scores.items():\n",
                "            if comp not in all_scores:\n",
                "                all_scores[comp] = []\n",
                "            all_scores[comp].append(score)\n",
                "\n",
                "print(f\"\\nCompleted {valid_pairs}/{len(all_pairs)} valid pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute averages and sort\n",
                "avg_restoration = {comp: np.mean(scores) for comp, scores in all_scores.items()}\n",
                "\n",
                "# Separate heads and MLPs\n",
                "head_scores = {k: v for k, v in avg_restoration.items() if 'H' in k}\n",
                "mlp_scores = {k: v for k, v in avg_restoration.items() if 'MLP' in k}\n",
                "\n",
                "sorted_heads = sorted(head_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "sorted_mlps = sorted(mlp_scores.items(), key=lambda x: x[1], reverse=True)\n",
                "\n",
                "print(\"TOP 10 FAITHFUL HEADS (highest restoration):\")\n",
                "print(\"-\" * 40)\n",
                "for comp, score in sorted_heads[:10]:\n",
                "    print(f\"  {comp}: {score:+.3f}\")\n",
                "\n",
                "print(\"\\nBOTTOM 10 HEADS (potential shortcuts):\")\n",
                "print(\"-\" * 40)\n",
                "for comp, score in sorted_heads[-10:]:\n",
                "    print(f\"  {comp}: {score:+.3f}\")\n",
                "\n",
                "print(\"\\nMLP SCORES:\")\n",
                "print(\"-\" * 40)\n",
                "for comp, score in sorted_mlps:\n",
                "    label = \"FAITHFUL\" if score > 0.3 else \"SHORTCUT\" if score < 0 else \"\"\n",
                "    print(f\"  {comp}: {score:+.3f}  {label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize: Heatmap of head restoration scores\n",
                "head_matrix = np.zeros((N_LAYERS, N_HEADS))\n",
                "for layer in range(N_LAYERS):\n",
                "    for head in range(N_HEADS):\n",
                "        key = f\"L{layer}H{head}\"\n",
                "        head_matrix[layer, head] = head_scores.get(key, 0)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Head heatmap\n",
                "im = axes[0].imshow(head_matrix, cmap='RdBu_r', aspect='auto', vmin=-0.5, vmax=0.5)\n",
                "axes[0].set_xlabel('Head')\n",
                "axes[0].set_ylabel('Layer')\n",
                "axes[0].set_title('Head-Level Restoration Scores\\n(Blue=Shortcut, Red=Faithful)')\n",
                "plt.colorbar(im, ax=axes[0], label='Restoration Score')\n",
                "\n",
                "# Add text annotations for extreme values\n",
                "for layer in range(N_LAYERS):\n",
                "    for head in range(N_HEADS):\n",
                "        val = head_matrix[layer, head]\n",
                "        if abs(val) > 0.2:\n",
                "            axes[0].text(head, layer, f'{val:.1f}', ha='center', va='center', \n",
                "                        fontsize=6, color='white' if abs(val) > 0.3 else 'black')\n",
                "\n",
                "# MLP bar chart\n",
                "mlp_vals = [mlp_scores.get(f\"L{l}MLP\", 0) for l in range(N_LAYERS)]\n",
                "colors = ['#d62728' if v > 0.1 else '#1f77b4' if v < -0.05 else '#888888' for v in mlp_vals]\n",
                "axes[1].barh(range(N_LAYERS), mlp_vals, color=colors, alpha=0.8)\n",
                "axes[1].set_xlabel('Restoration Score')\n",
                "axes[1].set_ylabel('Layer')\n",
                "axes[1].set_title('MLP Restoration Scores')\n",
                "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
                "axes[1].axvline(x=0.3, color='green', linestyle='--', linewidth=0.5, alpha=0.5, label='Faithful threshold')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(RESULTS_DIR / 'head_level_restoration.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved: {RESULTS_DIR / 'head_level_restoration.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Circuit Classification (Head-Level)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Classify components\n",
                "FAITHFUL_THRESHOLD = 0.2\n",
                "SHORTCUT_THRESHOLD = -0.05\n",
                "\n",
                "faithful_heads = [(k, v) for k, v in sorted_heads if v >= FAITHFUL_THRESHOLD]\n",
                "shortcut_heads = [(k, v) for k, v in sorted_heads if v <= SHORTCUT_THRESHOLD]\n",
                "neutral_heads = [(k, v) for k, v in sorted_heads if SHORTCUT_THRESHOLD < v < FAITHFUL_THRESHOLD]\n",
                "\n",
                "faithful_mlps = [(k, v) for k, v in sorted_mlps if v >= FAITHFUL_THRESHOLD]\n",
                "shortcut_mlps = [(k, v) for k, v in sorted_mlps if v <= SHORTCUT_THRESHOLD]\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"CIRCUIT CLASSIFICATION (HEAD-LEVEL)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(f\"\\n✓ FAITHFUL HEADS ({len(faithful_heads)}) - Use CoT reasoning:\")\n",
                "for h, s in faithful_heads[:15]:\n",
                "    print(f\"    {h}: {s:+.3f}\")\n",
                "if len(faithful_heads) > 15:\n",
                "    print(f\"    ... and {len(faithful_heads) - 15} more\")\n",
                "\n",
                "print(f\"\\n✗ SHORTCUT HEADS ({len(shortcut_heads)}) - Bypass CoT:\")\n",
                "for h, s in shortcut_heads[:15]:\n",
                "    print(f\"    {h}: {s:+.3f}\")\n",
                "if len(shortcut_heads) > 15:\n",
                "    print(f\"    ... and {len(shortcut_heads) - 15} more\")\n",
                "\n",
                "print(f\"\\n✓ FAITHFUL MLPs ({len(faithful_mlps)}):\")\n",
                "for m, s in faithful_mlps:\n",
                "    print(f\"    {m}: {s:+.3f}\")\n",
                "\n",
                "print(f\"\\n✗ SHORTCUT MLPs ({len(shortcut_mlps)}):\")\n",
                "for m, s in shortcut_mlps:\n",
                "    print(f\"    {m}: {s:+.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results = {\n",
                "    'avg_restoration': avg_restoration,\n",
                "    'faithful_heads': [h for h, _ in faithful_heads],\n",
                "    'shortcut_heads': [h for h, _ in shortcut_heads],\n",
                "    'faithful_mlps': [m for m, _ in faithful_mlps],\n",
                "    'shortcut_mlps': [m for m, _ in shortcut_mlps],\n",
                "}\n",
                "\n",
                "import json\n",
                "with open(RESULTS_DIR / 'head_level_results.json', 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "print(f\"Saved results to {RESULTS_DIR / 'head_level_results.json'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Summary\n",
                "\n",
                "### Key Improvements\n",
                "1. **Head-level granularity**: Now know exactly which heads (e.g., L5H1) are faithful vs shortcuts\n",
                "2. **Token verification**: Logged all tokenizations to catch silent bugs\n",
                "3. **Shortcut detection pairs**: \"Wrong CoT → Correct Answer\" tests find true shortcuts\n",
                "\n",
                "### Next Steps\n",
                "- Phase 2: Use identified heads as features for faithfulness classifier\n",
                "- Phase 3: Circuit Breaker - suppress shortcut heads during inference"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}