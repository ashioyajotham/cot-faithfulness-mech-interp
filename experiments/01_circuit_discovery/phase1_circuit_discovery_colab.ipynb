{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 1: Circuit Discovery for Chain-of-Thought Reasoning\n",
                "\n",
                "**Colab-Native Implementation** | [GitHub](https://github.com/ashioyajotham/cot-faithfulness-mech-interp)\n",
                "\n",
                "This notebook applies mechanistic interpretability techniques to reverse-engineer the computational circuits underlying chain-of-thought (CoT) reasoning in GPT-2 Small.\n",
                "\n",
                "## What We'll Do\n",
                "1. **Load GPT-2** via TransformerLens with activation hooks\n",
                "2. **Generate reasoning examples** (arithmetic, logic, physics)\n",
                "3. **Cache activations** during forward passes\n",
                "4. **Build attribution graphs** mapping information flow\n",
                "5. **Run causal ablations** to identify critical circuit components\n",
                "6. **Visualize** attention patterns and circuit structure\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies\n",
                "\n",
                "‚ö†Ô∏è **IMPORTANT**: After running the install cell, you must **restart the runtime** before continuing.\n",
                "\n",
                "`Runtime > Restart runtime` (or Ctrl+M .)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (run once, then RESTART RUNTIME)\n",
                "!pip install transformer-lens==2.0.0 torch matplotlib networkx einops jaxtyping -q\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚úÖ Installation complete!\")\n",
                "print(\"‚ö†Ô∏è  NOW RESTART RUNTIME: Runtime > Restart runtime\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run this AFTER restarting runtime\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import networkx as nx\n",
                "import os\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# GPU Check\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "if device == \"cuda\":\n",
                "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name()}\")\n",
                "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è Running on CPU - this will be slow\")\n",
                "\n",
                "# Set up results directory\n",
                "RESULTS_DIR = Path(\"results/phase1_circuit_discovery\")\n",
                "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
                "print(f\"\\nüìÅ Results will be saved to: {RESULTS_DIR}\")\n",
                "\n",
                "# Set seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformer_lens import HookedTransformer\n",
                "\n",
                "# Load GPT-2 Small (124M params)\n",
                "model = HookedTransformer.from_pretrained(\n",
                "    \"gpt2\",\n",
                "    device=device,\n",
                "    fold_ln=False,           # Keep layer norms separate for cleaner analysis\n",
                "    center_writing_weights=False,\n",
                "    center_unembed=False\n",
                ")\n",
                "model.eval()\n",
                "\n",
                "print(f\"\\n‚úÖ Model loaded: {model.cfg.model_name}\")\n",
                "print(f\"   Layers: {model.cfg.n_layers}\")\n",
                "print(f\"   Heads per layer: {model.cfg.n_heads}\")\n",
                "print(f\"   Hidden dim: {model.cfg.d_model}\")\n",
                "print(f\"   Vocab size: {model.cfg.d_vocab}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Reasoning Task Generation\n",
                "\n",
                "We create prompts that elicit chain-of-thought reasoning across three domains:\n",
                "- **Arithmetic**: Multi-step calculations\n",
                "- **Logic**: Transitive reasoning\n",
                "- **Physics**: Qualitative reasoning about physical systems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ReasoningTask:\n",
                "    \"\"\"A reasoning task with prompt and expected answer.\"\"\"\n",
                "    prompt: str\n",
                "    task_type: str\n",
                "    expected_answer: Optional[str] = None\n",
                "\n",
                "def generate_arithmetic_task() -> ReasoningTask:\n",
                "    \"\"\"Generate a simple arithmetic reasoning task.\"\"\"\n",
                "    a, b = np.random.randint(10, 50, size=2)\n",
                "    prompt = f\"\"\"Question: What is {a} + {b}?\n",
                "Let me think step by step.\n",
                "First, I'll add the ones place: {a % 10} + {b % 10} = {(a % 10) + (b % 10)}.\n",
                "Then, I'll add the tens place: {a // 10} + {b // 10} = {(a // 10) + (b // 10)}.\n",
                "Combining these, the answer is\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"arithmetic\", expected_answer=str(a + b))\n",
                "\n",
                "def generate_logic_task() -> ReasoningTask:\n",
                "    \"\"\"Generate a transitive logic reasoning task.\"\"\"\n",
                "    names = [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"]\n",
                "    a, b, c = np.random.choice(names, size=3, replace=False)\n",
                "    prompt = f\"\"\"Question: If {a} is taller than {b}, and {b} is taller than {c}, who is the tallest?\n",
                "Let me reason through this.\n",
                "{a} > {b} (given)\n",
                "{b} > {c} (given)\n",
                "By transitivity, {a} > {b} > {c}.\n",
                "Therefore, the tallest is\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"logic\", expected_answer=a)\n",
                "\n",
                "def generate_physics_task() -> ReasoningTask:\n",
                "    \"\"\"Generate a simple physics reasoning task.\"\"\"\n",
                "    height = np.random.randint(5, 20)\n",
                "    prompt = f\"\"\"Question: A ball is dropped from {height} meters. Ignoring air resistance, will it speed up or slow down as it falls?\n",
                "Let me think about this.\n",
                "When an object falls, gravity pulls it downward.\n",
                "Gravity provides constant acceleration of about 9.8 m/s¬≤.\n",
                "Since acceleration is constant and positive, the speed will\"\"\"\n",
                "    return ReasoningTask(prompt=prompt, task_type=\"physics\", expected_answer=\"speed up\")\n",
                "\n",
                "# Generate sample tasks\n",
                "tasks = [\n",
                "    generate_arithmetic_task(),\n",
                "    generate_logic_task(),\n",
                "    generate_physics_task(),\n",
                "    generate_arithmetic_task(),\n",
                "]\n",
                "\n",
                "print(f\"Generated {len(tasks)} reasoning tasks:\")\n",
                "for i, task in enumerate(tasks, 1):\n",
                "    print(f\"\\n--- Task {i} ({task.task_type}) ---\")\n",
                "    print(task.prompt[:200] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generate Completions with Activation Caching\n",
                "\n",
                "We run each prompt through the model and cache activations for analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class ReasoningExample:\n",
                "    \"\"\"A completed reasoning example with cached activations.\"\"\"\n",
                "    task: ReasoningTask\n",
                "    full_text: str\n",
                "    generated_text: str\n",
                "    tokens: torch.Tensor\n",
                "    cache: Dict\n",
                "\n",
                "def generate_with_cache(task: ReasoningTask, max_new_tokens: int = 20) -> ReasoningExample:\n",
                "    \"\"\"Generate completion and cache activations (memory-efficient).\"\"\"\n",
                "    # Tokenize prompt\n",
                "    tokens = model.to_tokens(task.prompt)\n",
                "    \n",
                "    # Generate completion (without caching during generation for memory efficiency)\n",
                "    with torch.no_grad():\n",
                "        generated_tokens = model.generate(\n",
                "            tokens,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            stop_at_eos=True\n",
                "        )\n",
                "    \n",
                "    # Run one final forward pass with caching on the full sequence\n",
                "    with torch.no_grad():\n",
                "        _, cache = model.run_with_cache(\n",
                "            generated_tokens,\n",
                "            names_filter=lambda name: any(x in name for x in [\n",
                "                \"hook_attn_out\",      # Attention output\n",
                "                \"hook_mlp_out\",       # MLP output\n",
                "                \"attn.hook_pattern\",  # Attention patterns\n",
                "                \"hook_resid_post\"     # Residual stream\n",
                "            ])\n",
                "        )\n",
                "    \n",
                "    # Decode text\n",
                "    full_text = model.to_string(generated_tokens[0])\n",
                "    generated_text = full_text[len(task.prompt):]\n",
                "    \n",
                "    return ReasoningExample(\n",
                "        task=task,\n",
                "        full_text=full_text,\n",
                "        generated_text=generated_text,\n",
                "        tokens=generated_tokens,\n",
                "        cache=cache\n",
                "    )\n",
                "\n",
                "# Generate examples\n",
                "print(\"Generating reasoning examples with activation caching...\\n\")\n",
                "examples = []\n",
                "for i, task in enumerate(tasks, 1):\n",
                "    print(f\"Task {i}/{len(tasks)}: {task.task_type}...\", end=\" \")\n",
                "    example = generate_with_cache(task)\n",
                "    examples.append(example)\n",
                "    print(f\"‚úì Generated {len(example.generated_text)} chars\")\n",
                "\n",
                "print(f\"\\n‚úÖ Generated {len(examples)} examples with cached activations\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show a sample completion\n",
                "print(\"=\" * 60)\n",
                "print(\"SAMPLE COMPLETION\")\n",
                "print(\"=\" * 60)\n",
                "ex = examples[0]\n",
                "print(f\"Task Type: {ex.task.task_type}\")\n",
                "print(f\"Expected: {ex.task.expected_answer}\")\n",
                "print(f\"\\nFull Text:\\n{ex.full_text}\")\n",
                "print(f\"\\nCache Keys: {list(ex.cache.keys())[:5]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Attribution Graph Construction\n",
                "\n",
                "We build a directed graph where:\n",
                "- **Nodes** = attention heads and MLP layers\n",
                "- **Edges** = information flow weighted by activation magnitude"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class CircuitNode:\n",
                "    \"\"\"A node in the circuit graph.\"\"\"\n",
                "    node_id: str\n",
                "    layer: int\n",
                "    component_type: str  # \"attn\", \"mlp\", \"embed\", \"output\"\n",
                "    head_idx: Optional[int] = None\n",
                "    activation_norm: float = 0.0\n",
                "\n",
                "def build_attribution_graph(example: ReasoningExample, threshold: float = 0.1) -> nx.DiGraph:\n",
                "    \"\"\"\n",
                "    Build attribution graph from cached activations.\n",
                "    \n",
                "    Nodes: Attention heads and MLP layers\n",
                "    Edges: Weighted by activation magnitude (approximating information flow)\n",
                "    \"\"\"\n",
                "    G = nx.DiGraph()\n",
                "    cache = example.cache\n",
                "    n_layers = model.cfg.n_layers\n",
                "    n_heads = model.cfg.n_heads\n",
                "    \n",
                "    # Add embedding node\n",
                "    G.add_node(\"embed\", layer=-1, component_type=\"embed\", activation_norm=1.0)\n",
                "    \n",
                "    # Process each layer\n",
                "    for layer in range(n_layers):\n",
                "        # Get attention output for this layer\n",
                "        attn_key = f\"blocks.{layer}.hook_attn_out\"\n",
                "        mlp_key = f\"blocks.{layer}.hook_mlp_out\"\n",
                "        \n",
                "        if attn_key in cache:\n",
                "            attn_out = cache[attn_key]  # [batch, seq, d_model]\n",
                "            attn_norm = attn_out.norm(dim=-1).mean().item()\n",
                "            \n",
                "            # Add attention head nodes (using mean activation as proxy)\n",
                "            for head in range(n_heads):\n",
                "                node_id = f\"L{layer}H{head}\"\n",
                "                head_norm = attn_norm / n_heads  # Approximate\n",
                "                G.add_node(node_id, layer=layer, component_type=\"attn\", \n",
                "                          head_idx=head, activation_norm=head_norm)\n",
                "        \n",
                "        if mlp_key in cache:\n",
                "            mlp_out = cache[mlp_key]\n",
                "            mlp_norm = mlp_out.norm(dim=-1).mean().item()\n",
                "            node_id = f\"L{layer}MLP\"\n",
                "            G.add_node(node_id, layer=layer, component_type=\"mlp\", activation_norm=mlp_norm)\n",
                "    \n",
                "    # Add output node\n",
                "    G.add_node(\"output\", layer=n_layers, component_type=\"output\", activation_norm=1.0)\n",
                "    \n",
                "    # Build edges based on layer adjacency and activation strength\n",
                "    nodes_by_layer = {}\n",
                "    for node, data in G.nodes(data=True):\n",
                "        layer = data.get('layer', -1)\n",
                "        if layer not in nodes_by_layer:\n",
                "            nodes_by_layer[layer] = []\n",
                "        nodes_by_layer[layer].append((node, data))\n",
                "    \n",
                "    # Connect adjacent layers\n",
                "    layers = sorted(nodes_by_layer.keys())\n",
                "    for i in range(len(layers) - 1):\n",
                "        src_layer = layers[i]\n",
                "        dst_layer = layers[i + 1]\n",
                "        \n",
                "        for src_node, src_data in nodes_by_layer[src_layer]:\n",
                "            for dst_node, dst_data in nodes_by_layer[dst_layer]:\n",
                "                # Weight edge by destination activation (information received)\n",
                "                weight = dst_data.get('activation_norm', 0.1)\n",
                "                if weight > threshold:\n",
                "                    G.add_edge(src_node, dst_node, weight=weight)\n",
                "    \n",
                "    return G\n",
                "\n",
                "# Build graphs for all examples\n",
                "graphs = []\n",
                "for i, example in enumerate(examples, 1):\n",
                "    print(f\"Building graph {i}/{len(examples)}...\", end=\" \")\n",
                "    G = build_attribution_graph(example)\n",
                "    graphs.append(G)\n",
                "    print(f\"‚úì {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
                "\n",
                "print(f\"\\n‚úÖ Built {len(graphs)} attribution graphs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze graph statistics\n",
                "G = graphs[0]\n",
                "print(\"=\" * 40)\n",
                "print(\"ATTRIBUTION GRAPH STATISTICS\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Nodes: {G.number_of_nodes()}\")\n",
                "print(f\"Edges: {G.number_of_edges()}\")\n",
                "\n",
                "# Count by component type\n",
                "type_counts = {}\n",
                "for _, data in G.nodes(data=True):\n",
                "    t = data.get('component_type', 'unknown')\n",
                "    type_counts[t] = type_counts.get(t, 0) + 1\n",
                "print(f\"\\nBy Component Type: {type_counts}\")\n",
                "\n",
                "# Edge weight statistics\n",
                "weights = [d['weight'] for _, _, d in G.edges(data=True)]\n",
                "if weights:\n",
                "    print(f\"\\nEdge Weights:\")\n",
                "    print(f\"  Mean: {np.mean(weights):.3f}\")\n",
                "    print(f\"  Std:  {np.std(weights):.3f}\")\n",
                "    print(f\"  Max:  {np.max(weights):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Causal Ablation Analysis\n",
                "\n",
                "We systematically ablate (zero out) individual attention heads and MLP layers to identify which components are causally necessary for correct reasoning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_baseline_loss(tokens: torch.Tensor) -> float:\n",
                "    \"\"\"Get cross-entropy loss without any ablation.\"\"\"\n",
                "    with torch.no_grad():\n",
                "        logits = model(tokens)\n",
                "        # Loss on last token prediction\n",
                "        loss = torch.nn.functional.cross_entropy(\n",
                "            logits[0, -2, :],  # Predict last token from second-to-last\n",
                "            tokens[0, -1]\n",
                "        )\n",
                "    return loss.item()\n",
                "\n",
                "def ablate_attention_head(tokens: torch.Tensor, layer: int, head: int) -> float:\n",
                "    \"\"\"Ablate a specific attention head and measure effect on loss.\"\"\"\n",
                "    def head_ablation_hook(pattern, hook):\n",
                "        # Zero out the attention pattern for this head\n",
                "        pattern[:, head, :, :] = 0\n",
                "        return pattern\n",
                "    \n",
                "    hook_name = f\"blocks.{layer}.attn.hook_pattern\"\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        logits = model.run_with_hooks(\n",
                "            tokens,\n",
                "            fwd_hooks=[(hook_name, head_ablation_hook)]\n",
                "        )\n",
                "        loss = torch.nn.functional.cross_entropy(\n",
                "            logits[0, -2, :],\n",
                "            tokens[0, -1]\n",
                "        )\n",
                "    return loss.item()\n",
                "\n",
                "def ablate_mlp_layer(tokens: torch.Tensor, layer: int) -> float:\n",
                "    \"\"\"Ablate an entire MLP layer and measure effect on loss.\"\"\"\n",
                "    def mlp_ablation_hook(mlp_out, hook):\n",
                "        return torch.zeros_like(mlp_out)\n",
                "    \n",
                "    hook_name = f\"blocks.{layer}.hook_mlp_out\"\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        logits = model.run_with_hooks(\n",
                "            tokens,\n",
                "            fwd_hooks=[(hook_name, mlp_ablation_hook)]\n",
                "        )\n",
                "        loss = torch.nn.functional.cross_entropy(\n",
                "            logits[0, -2, :],\n",
                "            tokens[0, -1]\n",
                "        )\n",
                "    return loss.item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run ablation study on first example\n",
                "example = examples[0]\n",
                "tokens = example.tokens\n",
                "\n",
                "print(\"Running causal ablation study...\\n\")\n",
                "\n",
                "# Baseline\n",
                "baseline_loss = get_baseline_loss(tokens)\n",
                "print(f\"Baseline loss: {baseline_loss:.4f}\\n\")\n",
                "\n",
                "# Ablate attention heads\n",
                "head_effects = {}\n",
                "print(\"Ablating attention heads...\")\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    for head in range(model.cfg.n_heads):\n",
                "        ablated_loss = ablate_attention_head(tokens, layer, head)\n",
                "        effect = ablated_loss - baseline_loss\n",
                "        head_effects[f\"L{layer}H{head}\"] = effect\n",
                "\n",
                "# Ablate MLP layers\n",
                "mlp_effects = {}\n",
                "print(\"Ablating MLP layers...\")\n",
                "for layer in range(model.cfg.n_layers):\n",
                "    ablated_loss = ablate_mlp_layer(tokens, layer)\n",
                "    effect = ablated_loss - baseline_loss\n",
                "    mlp_effects[f\"L{layer}MLP\"] = effect\n",
                "\n",
                "print(\"\\n‚úÖ Ablation study complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze ablation results\n",
                "all_effects = {**head_effects, **mlp_effects}\n",
                "\n",
                "# Sort by effect magnitude\n",
                "sorted_effects = sorted(all_effects.items(), key=lambda x: abs(x[1]), reverse=True)\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"TOP 15 MOST IMPORTANT COMPONENTS (by causal effect)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"{'Component':<12} {'Effect':>10} {'Direction':<10}\")\n",
                "print(\"-\" * 32)\n",
                "for comp, effect in sorted_effects[:15]:\n",
                "    direction = \"‚Üë loss\" if effect > 0 else \"‚Üì loss\"\n",
                "    print(f\"{comp:<12} {effect:>10.4f} {direction}\")\n",
                "\n",
                "print(\"\\nüí° Positive effect = ablating hurts performance (component is important)\")\n",
                "print(\"   Negative effect = ablating helps performance (component may be harmful)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot ablation effects as heatmap\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Attention head effects heatmap\n",
                "n_layers = model.cfg.n_layers\n",
                "n_heads = model.cfg.n_heads\n",
                "head_matrix = np.zeros((n_layers, n_heads))\n",
                "for layer in range(n_layers):\n",
                "    for head in range(n_heads):\n",
                "        head_matrix[layer, head] = head_effects.get(f\"L{layer}H{head}\", 0)\n",
                "\n",
                "im1 = axes[0].imshow(head_matrix, cmap='RdBu_r', aspect='auto')\n",
                "axes[0].set_xlabel('Head')\n",
                "axes[0].set_ylabel('Layer')\n",
                "axes[0].set_title('Attention Head Ablation Effects')\n",
                "plt.colorbar(im1, ax=axes[0], label='Œî Loss')\n",
                "\n",
                "# MLP effects bar chart\n",
                "mlp_layers = list(range(n_layers))\n",
                "mlp_values = [mlp_effects.get(f\"L{l}MLP\", 0) for l in mlp_layers]\n",
                "colors = ['red' if v > 0 else 'blue' for v in mlp_values]\n",
                "axes[1].barh(mlp_layers, mlp_values, color=colors, alpha=0.7)\n",
                "axes[1].set_xlabel('Œî Loss')\n",
                "axes[1].set_ylabel('Layer')\n",
                "axes[1].set_title('MLP Layer Ablation Effects')\n",
                "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "save_path = RESULTS_DIR / 'ablation_effects.png'\n",
                "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"\\nüìä Saved: {save_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention patterns for a key head\n",
                "top_head = sorted_effects[0][0]  # Most important component\n",
                "\n",
                "if top_head.startswith(\"L\") and \"H\" in top_head and \"MLP\" not in top_head:\n",
                "    # Parse layer and head\n",
                "    parts = top_head.replace(\"L\", \"\").split(\"H\")\n",
                "    layer, head = int(parts[0]), int(parts[1])\n",
                "    \n",
                "    # Get attention pattern\n",
                "    pattern_key = f\"blocks.{layer}.attn.hook_pattern\"\n",
                "    if pattern_key in example.cache:\n",
                "        pattern = example.cache[pattern_key][0, head].cpu().numpy()  # [seq, seq]\n",
                "        \n",
                "        # Get tokens for labels\n",
                "        token_strs = model.to_str_tokens(example.tokens[0])\n",
                "        \n",
                "        # Truncate if too long\n",
                "        max_len = 30\n",
                "        if len(token_strs) > max_len:\n",
                "            pattern = pattern[:max_len, :max_len]\n",
                "            token_strs = token_strs[:max_len]\n",
                "        \n",
                "        plt.figure(figsize=(12, 10))\n",
                "        plt.imshow(pattern, cmap='Blues')\n",
                "        plt.colorbar(label='Attention Weight')\n",
                "        plt.xticks(range(len(token_strs)), token_strs, rotation=90, fontsize=8)\n",
                "        plt.yticks(range(len(token_strs)), token_strs, fontsize=8)\n",
                "        plt.xlabel('Key Position')\n",
                "        plt.ylabel('Query Position')\n",
                "        plt.title(f'Attention Pattern: {top_head} (Most Important Head)')\n",
                "        plt.tight_layout()\n",
                "        save_path = RESULTS_DIR / 'attention_pattern.png'\n",
                "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "        plt.show()\n",
                "        print(f\"\\nüìä Saved: {save_path}\")\n",
                "else:\n",
                "    print(f\"Top component is {top_head} (not an attention head)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Draw circuit graph (simplified view)\n",
                "G = graphs[0]\n",
                "\n",
                "# Create a simplified graph with only high-effect nodes\n",
                "important_nodes = [comp for comp, effect in sorted_effects[:20] if abs(effect) > 0.01]\n",
                "important_nodes = [n for n in important_nodes if n in G.nodes()]\n",
                "\n",
                "# Add connections\n",
                "subgraph_nodes = set(important_nodes)\n",
                "for node in important_nodes:\n",
                "    # Add predecessors/successors within 1 hop\n",
                "    subgraph_nodes.update(G.predecessors(node))\n",
                "    subgraph_nodes.update(G.successors(node))\n",
                "\n",
                "H = G.subgraph(subgraph_nodes)\n",
                "\n",
                "plt.figure(figsize=(14, 10))\n",
                "\n",
                "# Layout by layer\n",
                "pos = {}\n",
                "for node, data in H.nodes(data=True):\n",
                "    layer = data.get('layer', 0)\n",
                "    # Spread nodes horizontally within layer\n",
                "    nodes_in_layer = [n for n, d in H.nodes(data=True) if d.get('layer', 0) == layer]\n",
                "    idx = nodes_in_layer.index(node)\n",
                "    pos[node] = (idx - len(nodes_in_layer)/2, -layer)\n",
                "\n",
                "# Color by component type\n",
                "colors = []\n",
                "for node, data in H.nodes(data=True):\n",
                "    if data.get('component_type') == 'attn':\n",
                "        colors.append('#4CAF50' if node in important_nodes else '#A5D6A7')\n",
                "    elif data.get('component_type') == 'mlp':\n",
                "        colors.append('#2196F3' if node in important_nodes else '#90CAF9')\n",
                "    else:\n",
                "        colors.append('#9E9E9E')\n",
                "\n",
                "nx.draw(H, pos, node_color=colors, with_labels=True, font_size=7,\n",
                "        node_size=500, arrows=True, arrowsize=10, edge_color='#BDBDBD')\n",
                "\n",
                "plt.title('Circuit Graph (Top 20 Important Components + Neighbors)')\n",
                "plt.tight_layout()\n",
                "save_path = RESULTS_DIR / 'circuit_graph.png'\n",
                "plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"\\nüìä Saved: {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Summary & Key Findings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"CIRCUIT DISCOVERY SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Find most important components\n",
                "top_5 = sorted_effects[:5]\n",
                "print(\"\\nüîë TOP 5 CRITICAL COMPONENTS:\")\n",
                "for i, (comp, effect) in enumerate(top_5, 1):\n",
                "    print(f\"   {i}. {comp}: Œî Loss = {effect:.4f}\")\n",
                "\n",
                "# Layer-level analysis\n",
                "layer_importance = {}\n",
                "for comp, effect in all_effects.items():\n",
                "    layer = int(comp.split('L')[1].split('H')[0].split('M')[0])\n",
                "    layer_importance[layer] = layer_importance.get(layer, 0) + abs(effect)\n",
                "\n",
                "sorted_layers = sorted(layer_importance.items(), key=lambda x: x[1], reverse=True)\n",
                "print(\"\\nüìä MOST IMPORTANT LAYERS:\")\n",
                "for layer, importance in sorted_layers[:3]:\n",
                "    print(f\"   Layer {layer}: Total |effect| = {importance:.4f}\")\n",
                "\n",
                "# Attention vs MLP\n",
                "attn_total = sum(abs(e) for c, e in all_effects.items() if 'H' in c and 'MLP' not in c)\n",
                "mlp_total = sum(abs(e) for c, e in all_effects.items() if 'MLP' in c)\n",
                "print(f\"\\n‚öñÔ∏è COMPONENT TYPE COMPARISON:\")\n",
                "print(f\"   Attention heads: {attn_total:.4f} total effect\")\n",
                "print(f\"   MLP layers: {mlp_total:.4f} total effect\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"‚úÖ Phase 1 Complete!\")\n",
                "print(f\"\\nüìÅ Results saved to: {RESULTS_DIR}\")\n",
                "print(\"   - ablation_effects.png\")\n",
                "print(\"   - attention_pattern.png\")\n",
                "print(\"   - circuit_graph.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "1. **Phase 2: Faithfulness Detection** - Train classifiers on circuit features to detect unfaithful reasoning\n",
                "2. **Phase 3: Targeted Interventions** - Test circuit modifications to improve reasoning faithfulness\n",
                "3. **Scale Analysis** - Extend to larger models (GPT-2 Medium/Large)\n",
                "\n",
                "---\n",
                "\n",
                "*Notebook by [Ashioya Jotham Victor](https://github.com/ashioyajotham)*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}