{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2C: Hybrid Approach & Ablation Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **Path C**: combining circuit discovery (Phase 1.5) with steering vectors.\n",
    "\n",
    "### The Hypothesis That Failed\n",
    "\n",
    "```\n",
    "Expected: Path C (Hybrid) > Path B (Full Steering)\n",
    "\n",
    "Reasoning:\n",
    "- Phase 1 identified 23 key components\n",
    "- Using ONLY those should reduce noise\n",
    "- Therefore hybrid should beat full residual stream\n",
    "```\n",
    "\n",
    "### What Actually Happened\n",
    "\n",
    "| Method | Accuracy |\n",
    "|--------|----------|\n",
    "| Path A (Linear Probe) | **88.1%** \u2190 WINNER |\n",
    "| Path B (Full Steering) | 72.3% |\n",
    "| Path C (Hybrid) | 65.0% \u2190 WORST! |\n",
    "\n",
    "### Why Did Hybrid Fail?\n",
    "**Diff-of-means treats all dimensions equally.** The probe learned L7H6 matters MORE than others. Hybrid couldn't capture this non-uniform weighting.\n",
    "\n",
    "### This Notebook Contains\n",
    "1. Hybrid approach implementation\n",
    "2. Analysis of WHY it failed\n",
    "3. Ablation studies (single-component & leave-one-out)\n",
    "4. Lessons for future research\n",
    "\n",
    "*Author: Victor Ashioya | CoT Faithfulness Mech Interp*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install 'transformers>=4.40,<4.46' transformer-lens torch matplotlib scikit-learn einops jaxtyping -q\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "RESULTS_DIR = Path(\"results/phase2c_hybrid\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Loading GPT-2 Small...\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device=device,\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "N_LAYERS = model.cfg.n_layers\n",
    "N_HEADS = model.cfg.n_heads\n",
    "D_HEAD = model.cfg.d_head\n",
    "D_MODEL = model.cfg.d_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1.5 Components & Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "FAITHFUL_HEADS = [\"L0H1\", \"L0H6\", \"L1H7\", \"L10H2\", \"L3H0\", \"L9H9\"]\n",
    "SHORTCUT_HEADS = [\"L7H6\", \"L2H10\", \"L0H3\", \"L2H0\", \"L3H10\", \"L0H10\", \"L6H8\", \"L4H7\", \"L5H9\", \"L0H0\"]\n",
    "FAITHFUL_MLPS = [\"L0MLP\", \"L5MLP\"]\n",
    "SHORTCUT_MLPS = [\"L10MLP\", \"L3MLP\", \"L2MLP\", \"L6MLP\", \"L4MLP\"]\n",
    "\n",
    "KEY_COMPONENTS = FAITHFUL_HEADS + SHORTCUT_HEADS + FAITHFUL_MLPS + SHORTCUT_MLPS\n",
    "\n",
    "COMPONENT_TYPES = {}\n",
    "for h in FAITHFUL_HEADS:\n",
    "    COMPONENT_TYPES[h] = \"faithful\"\n",
    "for h in SHORTCUT_HEADS:\n",
    "    COMPONENT_TYPES[h] = \"shortcut\"\n",
    "for m in FAITHFUL_MLPS:\n",
    "    COMPONENT_TYPES[m] = \"faithful\"\n",
    "for m in SHORTCUT_MLPS:\n",
    "    COMPONENT_TYPES[m] = \"shortcut\"\n",
    "\n",
    "print(f\"Key components: {len(KEY_COMPONENTS)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class FaithfulnessExample:\n",
    "    prompt: str\n",
    "    label: int\n",
    "    correct_answer: str\n",
    "    cot_answer: str\n",
    "    example_type: str\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "def generate_arithmetic_dataset(n_pairs: int = 400, seed: int = 42) -> Tuple[List, List]:\n",
    "    np.random.seed(seed)\n",
    "    faithful, unfaithful = [], []\n",
    "    \n",
    "    for i in range(n_pairs):\n",
    "        a = np.random.randint(10, 50)\n",
    "        b = np.random.randint(10, 50)\n",
    "        correct = a + b\n",
    "        \n",
    "        a_units, a_tens = a % 10, a // 10\n",
    "        b_units, b_tens = b % 10, b // 10\n",
    "        units_sum = a_units + b_units\n",
    "        tens_sum = a_tens + b_tens\n",
    "        \n",
    "        faithful_prompt = (\n",
    "            f\"Q: What is {a}+{b}?\\n\"\n",
    "            f\"Steps: units={a_units}+{b_units}={units_sum}, tens={a_tens}+{b_tens}={tens_sum}.\\n\"\n",
    "            f\"A:\"\n",
    "        )\n",
    "        faithful.append(FaithfulnessExample(\n",
    "            prompt=faithful_prompt, label=0, correct_answer=str(correct),\n",
    "            cot_answer=str(correct), example_type=\"faithful_addition\",\n",
    "            metadata={\"a\": a, \"b\": b, \"pair_id\": i}\n",
    "        ))\n",
    "        \n",
    "        wrong_units = units_sum + np.random.choice([3, 5, 7, -3, -5])\n",
    "        wrong_tens = tens_sum + np.random.choice([2, 4, -2, -4])\n",
    "        wrong_cot_answer = wrong_tens * 10 + wrong_units\n",
    "        \n",
    "        unfaithful_prompt = (\n",
    "            f\"Q: What is {a}+{b}?\\n\"\n",
    "            f\"Steps: units={a_units}+{b_units}={wrong_units}, tens={a_tens}+{b_tens}={wrong_tens}.\\n\"\n",
    "            f\"A:\"\n",
    "        )\n",
    "        unfaithful.append(FaithfulnessExample(\n",
    "            prompt=unfaithful_prompt, label=1, correct_answer=str(correct),\n",
    "            cot_answer=str(wrong_cot_answer), example_type=\"unfaithful_addition\",\n",
    "            metadata={\"a\": a, \"b\": b, \"pair_id\": i, \"wrong_answer\": wrong_cot_answer}\n",
    "        ))\n",
    "    \n",
    "    return faithful, unfaithful\n",
    "\n",
    "\n",
    "faithful_data, unfaithful_data = generate_arithmetic_dataset(n_pairs=400)\n",
    "all_data = faithful_data + unfaithful_data\n",
    "np.random.shuffle(all_data)\n",
    "\n",
    "print(f\"Dataset: {len(all_data)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_key_component_activations(\n",
    "    examples: List[FaithfulnessExample],\n",
    "    components: List[str],\n",
    "    verbose: bool = True\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Extract activations from key components only.\"\"\"\n",
    "    \n",
    "    all_activations = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for idx, example in enumerate(examples):\n",
    "        if verbose and idx % 100 == 0:\n",
    "            print(f\"  Processing {idx}/{len(examples)}...\")\n",
    "        \n",
    "        tokens = model.to_tokens(example.prompt)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(\n",
    "                tokens,\n",
    "                names_filter=lambda n: \"hook_z\" in n or \"hook_mlp_out\" in n\n",
    "            )\n",
    "        \n",
    "        example_acts = []\n",
    "        for comp in components:\n",
    "            if comp.endswith(\"MLP\"):\n",
    "                layer = int(comp[1:-3])\n",
    "                hook_name = f\"blocks.{layer}.hook_mlp_out\"\n",
    "                acts = cache[hook_name][0, -1, :].cpu().numpy()\n",
    "            else:\n",
    "                layer = int(comp.split(\"H\")[0][1:])\n",
    "                head = int(comp.split(\"H\")[1])\n",
    "                hook_name = f\"blocks.{layer}.attn.hook_z\"\n",
    "                acts = cache[hook_name][0, -1, head, :].cpu().numpy()\n",
    "            \n",
    "            example_acts.append(acts)\n",
    "        \n",
    "        all_activations.append(np.concatenate(example_acts))\n",
    "        all_labels.append(example.label)\n",
    "        \n",
    "        del cache\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return np.array(all_activations), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part 1: Hybrid Approach (Diff-of-Means on Key Components)\n",
    "\n",
    "Compute diff-of-means using **only** Phase 1.5 identified components instead of the full residual stream.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PART 1: HYBRID APPROACH\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nExtracting activations from key components...\")\n",
    "X, y = extract_key_component_activations(all_data, KEY_COMPONENTS)\n",
    "print(f\"Shape: {X.shape}\")\n",
    "\n",
    "# Split data\n",
    "train_mask = np.array([i < 400 for i in range(len(all_data))])\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "# Compute diff-of-means on key components\n",
    "faithful_acts = X_train[y_train == 0]\n",
    "unfaithful_acts = X_train[y_train == 1]\n",
    "\n",
    "hybrid_vector = faithful_acts.mean(axis=0) - unfaithful_acts.mean(axis=0)\n",
    "hybrid_vector_normalized = hybrid_vector / np.linalg.norm(hybrid_vector)\n",
    "\n",
    "print(f\"\\nHybrid vector computed:\")\n",
    "print(f\"  Shape: {hybrid_vector.shape}\")\n",
    "print(f\"  Norm: {np.linalg.norm(hybrid_vector):.4f}\")\n",
    "\n",
    "# Detection via projection\n",
    "test_scores = X_test @ hybrid_vector_normalized\n",
    "fpr, tpr, thresholds = roc_curve(y_test, -test_scores)\n",
    "roc_auc = roc_auc_score(y_test, -test_scores)\n",
    "\n",
    "# Find best threshold\n",
    "best_acc = 0\n",
    "for thresh in thresholds:\n",
    "    preds = (-test_scores > thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "print(f\"\\nHybrid Detection Results:\")\n",
    "print(f\"  ROC-AUC: {roc_auc:.3f}\")\n",
    "print(f\"  Best Accuracy: {best_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Part 2: Why Did Hybrid Fail?\n",
    "\n",
    "The hybrid approach failed because diff-of-means computes a **single direction** that treats all dimensions equally.\n",
    "It cannot learn that some components (like L7H6) are more important than others.\n",
    "\n",
    "We prove this by comparing:\n",
    "1. Diff-of-means vector weights (uniform)\n",
    "2. Linear probe learned weights (selective)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PART 2: WHY DID HYBRID FAIL?\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "\"\"\"\n",
    "The hybrid approach failed because diff-of-means computes a SINGLE direction\n",
    "that treats all dimensions equally. It cannot learn that some components\n",
    "(like L7H6) are more important than others.\n",
    "\n",
    "Let's prove this by comparing:\n",
    "1. Diff-of-means vector weights\n",
    "2. Linear probe learned weights\n",
    "\"\"\"\n",
    "\n",
    "# Train linear probe for comparison\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "probe_acc = accuracy_score(y_test, clf.predict(X_test_scaled))\n",
    "probe_auc = roc_auc_score(y_test, clf.predict_proba(X_test_scaled)[:, 1])\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Linear Probe Accuracy: {probe_acc:.3f}\")\n",
    "print(f\"  Hybrid Accuracy:       {best_acc:.3f}\")\n",
    "print(f\"  Gap:                   {probe_acc - best_acc:.3f}\")\n",
    "\n",
    "# Analyze weight distributions\n",
    "print(f\"\\n--- Weight Analysis ---\")\n",
    "\n",
    "# Get component-level weights for both methods\n",
    "def get_component_weights(vector, components):\n",
    "    \"\"\"Aggregate vector weights by component.\"\"\"\n",
    "    weights = {}\n",
    "    start_idx = 0\n",
    "    for comp in components:\n",
    "        if comp.endswith(\"MLP\"):\n",
    "            dim = D_MODEL\n",
    "        else:\n",
    "            dim = D_HEAD\n",
    "        \n",
    "        comp_weights = vector[start_idx:start_idx + dim]\n",
    "        weights[comp] = np.mean(np.abs(comp_weights))\n",
    "        start_idx += dim\n",
    "    return weights\n",
    "\n",
    "hybrid_weights = get_component_weights(hybrid_vector_normalized, KEY_COMPONENTS)\n",
    "probe_weights = get_component_weights(clf.coef_[0], KEY_COMPONENTS)\n",
    "\n",
    "# Normalize for comparison\n",
    "max_hybrid = max(hybrid_weights.values())\n",
    "max_probe = max(probe_weights.values())\n",
    "hybrid_weights_norm = {k: v/max_hybrid for k, v in hybrid_weights.items()}\n",
    "probe_weights_norm = {k: v/max_probe for k, v in probe_weights.items()}\n",
    "\n",
    "print(\"\\nComponent importance (normalized):\")\n",
    "print(f\"{'Component':<12} {'Hybrid':<10} {'Probe':<10} {'Diff':<10} {'Type':<10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for comp in KEY_COMPONENTS:\n",
    "    h_weight = hybrid_weights_norm[comp]\n",
    "    p_weight = probe_weights_norm[comp]\n",
    "    diff = p_weight - h_weight\n",
    "    comp_type = COMPONENT_TYPES[comp]\n",
    "    print(f\"{comp:<12} {h_weight:<10.3f} {p_weight:<10.3f} {diff:+<10.3f} {comp_type:<10}\")\n",
    "\n",
    "# Key finding: variance in weights\n",
    "hybrid_variance = np.var(list(hybrid_weights_norm.values()))\n",
    "probe_variance = np.var(list(probe_weights_norm.values()))\n",
    "\n",
    "print(f\"\\nWeight variance:\")\n",
    "print(f\"  Hybrid: {hybrid_variance:.4f} (more uniform)\")\n",
    "print(f\"  Probe:  {probe_variance:.4f} (more selective)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Part 3: Ablation Studies\n",
    "\n",
    "Two analyses:\n",
    "1. **Single-component**: Use only one component at a time \u2192 which is most informative?\n",
    "2. **Leave-one-out**: Remove one component \u2192 which causes biggest accuracy drop?\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PART 3: ABLATION STUDIES\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "\"\"\"\n",
    "To understand component contributions, we'll:\n",
    "1. Remove one component at a time and measure accuracy drop\n",
    "2. Use only one component at a time and measure accuracy\n",
    "\"\"\"\n",
    "\n",
    "# Single-component analysis\n",
    "print(\"\\nSingle-component detection accuracy:\")\n",
    "single_results = {}\n",
    "\n",
    "start_idx = 0\n",
    "for comp in KEY_COMPONENTS:\n",
    "    if comp.endswith(\"MLP\"):\n",
    "        dim = D_MODEL\n",
    "    else:\n",
    "        dim = D_HEAD\n",
    "    \n",
    "    X_single = X[:, start_idx:start_idx + dim]\n",
    "    X_train_s = X_single[train_mask]\n",
    "    X_test_s = X_single[test_mask]\n",
    "    \n",
    "    # Diff-of-means on single component\n",
    "    vec = X_train_s[y_train == 0].mean(0) - X_train_s[y_train == 1].mean(0)\n",
    "    vec_norm = vec / (np.linalg.norm(vec) + 1e-8)\n",
    "    \n",
    "    scores = X_test_s @ vec_norm\n",
    "    \n",
    "    # Try both directions\n",
    "    preds_pos = (scores > 0).astype(int)\n",
    "    preds_neg = (scores < 0).astype(int)\n",
    "    acc = max(accuracy_score(y_test, preds_pos), accuracy_score(y_test, preds_neg))\n",
    "    \n",
    "    single_results[comp] = acc\n",
    "    start_idx += dim\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_single = sorted(single_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Component':<12} {'Accuracy':<10} {'Type':<10}\")\n",
    "print(\"-\" * 32)\n",
    "for comp, acc in sorted_single[:10]:\n",
    "    comp_type = COMPONENT_TYPES[comp]\n",
    "    marker = \"\u2605\" if acc > 0.55 else \" \"\n",
    "    print(f\"{marker} {comp:<10} {acc:<10.3f} {comp_type:<10}\")\n",
    "\n",
    "# Leave-one-out analysis\n",
    "print(\"\\n\\nLeave-one-out analysis (drop in accuracy when removed):\")\n",
    "leave_one_out = {}\n",
    "\n",
    "for comp_to_remove in KEY_COMPONENTS[:10]:  # Top 10 to save time\n",
    "    remaining = [c for c in KEY_COMPONENTS if c != comp_to_remove]\n",
    "    \n",
    "    # Reconstruct X without this component\n",
    "    new_acts = []\n",
    "    start_idx = 0\n",
    "    for comp in KEY_COMPONENTS:\n",
    "        if comp.endswith(\"MLP\"):\n",
    "            dim = D_MODEL\n",
    "        else:\n",
    "            dim = D_HEAD\n",
    "        \n",
    "        if comp != comp_to_remove:\n",
    "            new_acts.append(X[:, start_idx:start_idx + dim])\n",
    "        start_idx += dim\n",
    "    \n",
    "    X_reduced = np.concatenate(new_acts, axis=1)\n",
    "    X_train_r = X_reduced[train_mask]\n",
    "    X_test_r = X_reduced[test_mask]\n",
    "    \n",
    "    # Train probe\n",
    "    scaler_r = StandardScaler()\n",
    "    X_train_r_scaled = scaler_r.fit_transform(X_train_r)\n",
    "    X_test_r_scaled = scaler_r.transform(X_test_r)\n",
    "    \n",
    "    clf_r = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    clf_r.fit(X_train_r_scaled, y_train)\n",
    "    \n",
    "    acc_reduced = accuracy_score(y_test, clf_r.predict(X_test_r_scaled))\n",
    "    drop = probe_acc - acc_reduced\n",
    "    leave_one_out[comp_to_remove] = drop\n",
    "\n",
    "sorted_loo = sorted(leave_one_out.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"{'Component':<12} {'Drop':<10} {'Type':<10}\")\n",
    "print(\"-\" * 32)\n",
    "for comp, drop in sorted_loo:\n",
    "    comp_type = COMPONENT_TYPES[comp]\n",
    "    print(f\"{comp:<12} {drop:+.3f}     {comp_type:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Part 4: Can We Fix Hybrid?\n",
    "\n",
    "Use probe weights to scale the diff-of-means: `weighted_hybrid = probe_weight[c] * diff_of_means[c]`\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PART 4: POTENTIAL IMPROVEMENTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "\"\"\"\n",
    "Could we fix hybrid by using probe weights to weight the diff-of-means?\n",
    "\n",
    "weighted_hybrid = sum(probe_weight[c] * diff_of_means[c] for c in components)\n",
    "\"\"\"\n",
    "\n",
    "# Weighted hybrid approach\n",
    "probe_coef = np.abs(clf.coef_[0])  # Use probe weights\n",
    "weighted_vector = hybrid_vector * probe_coef\n",
    "weighted_vector_normalized = weighted_vector / np.linalg.norm(weighted_vector)\n",
    "\n",
    "weighted_scores = X_test @ weighted_vector_normalized\n",
    "weighted_auc = roc_auc_score(y_test, -weighted_scores)\n",
    "\n",
    "# Find best threshold\n",
    "best_weighted_acc = 0\n",
    "for thresh in np.linspace(-2, 2, 100):\n",
    "    preds = (-weighted_scores > thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    if acc > best_weighted_acc:\n",
    "        best_weighted_acc = acc\n",
    "\n",
    "print(f\"Weighted Hybrid (using probe coefficients):\")\n",
    "print(f\"  Accuracy: {best_weighted_acc:.3f}\")\n",
    "print(f\"  ROC-AUC:  {weighted_auc:.3f}\")\n",
    "print(f\"  vs Original Hybrid: {best_weighted_acc - best_acc:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Method Comparison\n",
    "methods = ['Linear Probe', 'Full Steering', 'Hybrid', 'Weighted Hybrid']\n",
    "accuracies = [probe_acc, 0.723, best_acc, best_weighted_acc]  # 0.723 from Phase 2B\n",
    "colors = ['green', 'blue', 'red', 'orange']\n",
    "\n",
    "bars = axes[0, 0].bar(methods, accuracies, color=colors, alpha=0.7)\n",
    "axes[0, 0].axhline(y=0.8, color='black', linestyle='--', label='Target (80%)')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Method Comparison')\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].legend()\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, acc + 0.02, f'{acc:.1%}', \n",
    "                    ha='center', fontsize=10)\n",
    "\n",
    "# 2. Weight Distribution Comparison\n",
    "components_short = [c.replace('MLP', 'M') for c in KEY_COMPONENTS[:15]]\n",
    "x = np.arange(len(components_short))\n",
    "width = 0.35\n",
    "\n",
    "hybrid_vals = [hybrid_weights_norm[c] for c in KEY_COMPONENTS[:15]]\n",
    "probe_vals = [probe_weights_norm[c] for c in KEY_COMPONENTS[:15]]\n",
    "\n",
    "axes[0, 1].bar(x - width/2, hybrid_vals, width, label='Hybrid', alpha=0.7)\n",
    "axes[0, 1].bar(x + width/2, probe_vals, width, label='Probe', alpha=0.7)\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(components_short, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Normalized Weight')\n",
    "axes[0, 1].set_title('Weight Distribution: Hybrid vs Probe')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Single Component Accuracy\n",
    "top_10_comps = [c for c, _ in sorted_single[:10]]\n",
    "top_10_accs = [a for _, a in sorted_single[:10]]\n",
    "top_10_colors = ['green' if COMPONENT_TYPES[c] == 'faithful' else 'red' for c in top_10_comps]\n",
    "\n",
    "axes[1, 0].barh(range(len(top_10_comps)), top_10_accs, color=top_10_colors, alpha=0.7)\n",
    "axes[1, 0].set_yticks(range(len(top_10_comps)))\n",
    "axes[1, 0].set_yticklabels(top_10_comps)\n",
    "axes[1, 0].axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Single-Component Accuracy')\n",
    "axes[1, 0].set_title('Best Single Components\\n(Green=Faithful, Red=Shortcut)')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Leave-One-Out Impact\n",
    "loo_comps = [c for c, _ in sorted_loo]\n",
    "loo_drops = [d for _, d in sorted_loo]\n",
    "loo_colors = ['green' if COMPONENT_TYPES[c] == 'faithful' else 'red' for c in loo_comps]\n",
    "\n",
    "axes[1, 1].barh(range(len(loo_comps)), loo_drops, color=loo_colors, alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(loo_comps)))\n",
    "axes[1, 1].set_yticklabels(loo_comps)\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Accuracy Drop When Removed')\n",
    "axes[1, 1].set_title('Leave-One-Out Importance')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'phase2c_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = {\n",
    "    'hybrid': {\n",
    "        'accuracy': best_acc,\n",
    "        'roc_auc': roc_auc,\n",
    "    },\n",
    "    'probe': {\n",
    "        'accuracy': probe_acc,\n",
    "        'roc_auc': probe_auc,\n",
    "    },\n",
    "    'weighted_hybrid': {\n",
    "        'accuracy': best_weighted_acc,\n",
    "        'roc_auc': weighted_auc,\n",
    "    },\n",
    "    'single_component': {k: float(v) for k, v in sorted_single},\n",
    "    'leave_one_out': {k: float(v) for k, v in sorted_loo},\n",
    "    'weight_variance': {\n",
    "        'hybrid': float(hybrid_variance),\n",
    "        'probe': float(probe_variance),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'phase2c_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PHASE 2C COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# ============================================================================\n",
    "# KEY LESSONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "## Key Lessons from Hybrid Failure\n",
    "\n",
    "1. **Diff-of-means \u2260 Optimal classifier**\n",
    "   - Diff-of-means finds direction of maximum mean separation\n",
    "   - Linear probe finds direction of maximum class separation\n",
    "   - These are different objectives!\n",
    "\n",
    "2. **Sparsity isn't enough**\n",
    "   - Just using fewer components (sparsity) doesn't help if you\n",
    "     can't weight them appropriately\n",
    "   - Probe learns: L7H6 >> other components\n",
    "   - Hybrid treats: L7H6 = other components\n",
    "\n",
    "3. **When to use each method**:\n",
    "   - DETECTION \u2192 Linear probe (learns optimal weights)\n",
    "   - INTERVENTION \u2192 Diff-of-means (just need a direction)\n",
    "   \n",
    "4. **Weighted hybrid shows promise**\n",
    "   - Using probe weights to scale diff-of-means improves results\n",
    "   - This could combine benefits of both approaches\n",
    "\n",
    "5. **Most important component: L7H6 (shortcut head)**\n",
    "   - Highest probe weight\n",
    "   - Best single-component accuracy\n",
    "   - Largest leave-one-out impact\n",
    "   - This is the \"smoking gun\" for unfaithful reasoning\n",
    "\n",
    "## Implications for Future Work\n",
    "\n",
    "- For safety monitoring: Use probes, not steering vectors\n",
    "- For intervention: Steering vectors work, but probe weights\n",
    "  could inform which components to target\n",
    "- L7H6 ablation is the most promising intervention target\n",
    "\"\"\")\n",
    "\n",
    "np.save(RESULTS_DIR / 'hybrid_vector.npy', hybrid_vector)\n",
    "np.save(RESULTS_DIR / 'weighted_hybrid_vector.npy', weighted_vector)\n",
    "\n",
    "print(f\"\\n\u2713 Results saved to {RESULTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Lessons from Hybrid Failure\n",
    "\n",
    "1. **Diff-of-means \u2260 Optimal classifier** \u2014 finds max mean separation, not max class separation\n",
    "2. **Sparsity isn't enough** \u2014 Probe learns L7H6 >> others; Hybrid treats all equally\n",
    "3. **When to use each**:\n",
    "   - DETECTION \u2192 Linear probe (learns optimal weights)\n",
    "   - INTERVENTION \u2192 Diff-of-means (just need a direction)\n",
    "4. **L7H6 is the smoking gun** \u2014 highest probe weight, best single-component accuracy, largest leave-one-out impact\n",
    "\n",
    "### Implications\n",
    "- For safety monitoring: Use probes, not steering vectors\n",
    "- For intervention: Steering vectors work, but probe weights could target specific components\n",
    "- **L7H6 ablation** is the most promising intervention target\n",
    "\n",
    "*Phase 2C | Victor Ashioya | Bluedot Impact*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}