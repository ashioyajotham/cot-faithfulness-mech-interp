# Model configuration
model:
  name: "gpt2"  # GPT-2 Small (124M parameters)
  device: "cuda"  # or "cpu"
  max_length: 512
  cache_activations: true
  
# Sparse Autoencoder configuration
sae:
  layer_range: [0, 12]  # All GPT-2 Small layers
  expansion_factor: 4
  k_sparse: 64
  learning_rate: 1e-4
  batch_size: 32
  
# Training configuration
training:
  batch_size: 16
  learning_rate: 2e-5
  num_epochs: 5
  warmup_steps: 100
  max_grad_norm: 1.0
  seed: 42
  
# Attribution graphs configuration
attribution:
  top_k_features: 50
  prune_threshold: 0.1
  max_graph_depth: 5
  include_error_nodes: true
  
# Intervention configuration
interventions:
  strength_range: [-3.0, 3.0]
  num_strength_levels: 7
  intervention_layers: [6, 7, 8]  # Middle layers for faithfulness circuits
